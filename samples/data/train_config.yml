# Configs for model
model_type: TransformerModel
model_configs:
  vocab_size: 26029
  d_model: 512
  nhead: 1
  dim_feedforward: 1024
  num_layers: 4
  embedding_dim: 512
  dropout: 0.2
  cossim_threshold: 0.5
  activation: "gelu"

# Configs for data
data_loader_path: "NSP_sample.dill"

# Configs for training
epoch: 5
batch_size: 768
val_batch_size: 6144
learning_rate: 1.e-4
learning_rate_min: 0.0
optimizer: "AdamW"
steps_per_log: 100
steps_per_eval: 500

output_dir: "experiment"

optimizer_args: 
  weight_decay: 1.e-5
  eps: 1.e-12
