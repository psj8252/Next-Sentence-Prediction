# Configs for model
model_type: TransformerModel
model_configs:
  vocab_size: 10000
  d_model: 512
  nhead: 4
  dim_feedforward: 1024
  num_layers: 4
  dropout: 0.2
  activation: "relu"

# Configs for data
data_loader_path: "NSP_sample.dill"

# Configs for training
epoch: 10
batch_size: 64
val_batch_size: 512
learning_rate: 1.e-4
optimizer: "SGD"
steps_per_log: 1
steps_per_eval: 1

output_dir: "experiment"

optimizer_args: 
  momentum: 0.9
